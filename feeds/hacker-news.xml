<?xml version='1.0' encoding='utf-8'?>
<rss version="2.0"><channel><title>Hacker News Top Stories</title><link>https://news.ycombinator.com/</link><description>Top stories from Hacker News</description><language>en-us</language><lastBuildDate>Wed, 17 Dec 2025 05:06:51 +0000</lastBuildDate><item><title>alpr.watch</title><link>https://news.ycombinator.com/item?id=46290916</link><guid isPermaLink="true">https://news.ycombinator.com/item?id=46290916</guid><pubDate>Tue, 16 Dec 2025 16:54:19 +0000</pubDate><description>&lt;p&gt;&lt;strong&gt;Source:&lt;/strong&gt; &lt;a href="https://alpr.watch/"&gt;alpr.watch&lt;/a&gt;&lt;/p&gt;
                    &lt;hr/&gt;
                    &lt;div&gt;&lt;b&gt;Top Comment:&lt;/b&gt;&lt;/div&gt;
                    &lt;blockquote style="margin:1.5em 0; padding:1em 1.5em; border-left:4px solid; font-style:italic;"&gt;
                    &lt;p&gt;For years I&amp;#x27;ve thought about doing an &amp;quot;art project&amp;quot; to make people more aware of the fact they are being observed – but I never actually got up and did it.&lt;p&gt;The idea was to seek spots in the city where public web cams are pointed at, and paint QR codes on the ground at those spots (using a template), linking to the camera stream. So when curious passerbys scan the code, they see themselves in a camera stream and feel &amp;quot;watched&amp;quot;.&lt;/p&gt;
                    &lt;/blockquote&gt;
                    &lt;p&gt;&lt;strong&gt;Points:&lt;/strong&gt; 688 | &lt;strong&gt;Author:&lt;/strong&gt; theamk&lt;/p&gt;&lt;p&gt;&lt;a href="https://news.ycombinator.com/item?id=46290916"&gt;View All Comments&lt;/a&gt;&lt;/p&gt;</description><hn_points>688</hn_points><hn_author>theamk</hn_author><hn_order>1</hn_order></item><item><title>Pricing Changes for GitHub Actions</title><link>https://news.ycombinator.com/item?id=46291156</link><guid isPermaLink="true">https://news.ycombinator.com/item?id=46291156</guid><pubDate>Tue, 16 Dec 2025 17:12:02 +0000</pubDate><description>&lt;p&gt;&lt;strong&gt;Source:&lt;/strong&gt; &lt;a href="https://resources.github.com/actions/2026-pricing-changes-for-github-actions/"&gt;resources.github.com&lt;/a&gt;&lt;/p&gt;
                    &lt;hr/&gt;
                    &lt;div&gt;&lt;b&gt;Top Comment:&lt;/b&gt;&lt;/div&gt;
                    &lt;blockquote style="margin:1.5em 0; padding:1em 1.5em; border-left:4px solid; font-style:italic;"&gt;
                    &lt;p&gt;It is us, developers, who convinced our management to purchase GitHub Enterprise to be our forge. We didn&amp;#x27;t pay any heed to the values of software freedom. A closed source, proprietary software had good features. We saw that and convinced our management to purchase it. Never mind what cost it would impose in the future when the good software gets bad owners. Never mind that there were alternatives that were inferior but were community-developed, community-maintained and libre.&lt;p&gt;The writing is in the wall. First it was UX annoyances. Then it was GitHub Actions woes. Now it is paying money for running their software on your own hardware. It&amp;#x27;s only going to go downhill. Is it a good time now to learn from our mistakes and convince our teams and management to use community-maintained, libre alternatives? They may be inferior. They may lack features. But they&amp;#x27;re not going to pull user hostile tricks like this on you and me. And hey, if they are lacking features, maybe we should convince our management to let us contribute time to the community to add those features? It&amp;#x27;s a much better investment than sinking money into a software that will only grow more and more user hostile, isn&amp;#x27;t it?&lt;/p&gt;
                    &lt;/blockquote&gt;
                    &lt;p&gt;&lt;strong&gt;Points:&lt;/strong&gt; 550 | &lt;strong&gt;Author:&lt;/strong&gt; kevin-david&lt;/p&gt;&lt;p&gt;&lt;a href="https://news.ycombinator.com/item?id=46291156"&gt;View All Comments&lt;/a&gt;&lt;/p&gt;</description><hn_points>550</hn_points><hn_author>kevin-david</hn_author><hn_order>7</hn_order></item><item><title>No Graphics API</title><link>https://news.ycombinator.com/item?id=46293062</link><guid isPermaLink="true">https://news.ycombinator.com/item?id=46293062</guid><pubDate>Tue, 16 Dec 2025 19:20:17 +0000</pubDate><description>&lt;p&gt;&lt;strong&gt;Source:&lt;/strong&gt; &lt;a href="https://www.sebastianaaltonen.com/blog/no-graphics-api"&gt;www.sebastianaaltonen.com&lt;/a&gt;&lt;/p&gt;
                    &lt;hr/&gt;
                    &lt;div&gt;&lt;b&gt;Top Comment:&lt;/b&gt;&lt;/div&gt;
                    &lt;blockquote style="margin:1.5em 0; padding:1em 1.5em; border-left:4px solid; font-style:italic;"&gt;
                    &lt;p&gt;This is a fantastic article that demonstrates how many parts of vulkan and DX12 are no longer needed.&lt;p&gt;I hope the IHVs have a look at it because current DX12 seems semi abandoned, with it not supporting buffer pointers even when every gpu made on the last 10 (or more!) years can do pointers just fine, and while Vulkan doesnt do a 2.0 release that cleans things, so it carries a lot of baggage, and specially, tons of drivers that dont implement the extensions that really improve things.&lt;p&gt;If this api existed, you could emulate openGL on top of this faster than current opengl to vulkan layers, and something like SDL3 gpu would get a 3x&amp;#x2F;4x boost too.&lt;/p&gt;
                    &lt;/blockquote&gt;
                    &lt;p&gt;&lt;strong&gt;Points:&lt;/strong&gt; 490 | &lt;strong&gt;Author:&lt;/strong&gt; ryandrake&lt;/p&gt;&lt;p&gt;&lt;a href="https://news.ycombinator.com/item?id=46293062"&gt;View All Comments&lt;/a&gt;&lt;/p&gt;</description><hn_points>490</hn_points><hn_author>ryandrake</hn_author><hn_order>2</hn_order></item><item><title>Mozilla appoints new CEO Anthony Enzor-Demeo</title><link>https://news.ycombinator.com/item?id=46288491</link><guid isPermaLink="true">https://news.ycombinator.com/item?id=46288491</guid><pubDate>Tue, 16 Dec 2025 13:53:14 +0000</pubDate><description>&lt;p&gt;&lt;strong&gt;Source:&lt;/strong&gt; &lt;a href="https://blog.mozilla.org/en/mozilla/leadership/mozillas-next-chapter-anthony-enzor-demeo-new-ceo/"&gt;blog.mozilla.org&lt;/a&gt;&lt;/p&gt;
                    &lt;hr/&gt;
                    &lt;div&gt;&lt;b&gt;Top Comment:&lt;/b&gt;&lt;/div&gt;
                    &lt;blockquote style="margin:1.5em 0; padding:1em 1.5em; border-left:4px solid; font-style:italic;"&gt;
                    &lt;p&gt;Having worked at Mozilla a while ago, the CEO role is one I wouldn&amp;#x27;t wish on my worst enemy. Success is oddly defined: it&amp;#x27;s a non-profit (well, a for-profit owned by a non-profit) that needs to make a big profit in a short amount of time. And anything done to make that profit will annoy the community.&lt;p&gt;I hope Anthony leans into what makes Mozilla special. The past few years, Mozilla&amp;#x27;s business model has been to just meekly &amp;quot;us-too!&amp;quot; trends... IoT, Firefox OS, and more recently AI.&lt;p&gt;What Mozilla is good at, though, is taking complex things the average user doesn&amp;#x27;t really understand, and making it palpable and safe. They did this with web standards... nobody cared about web standards, but Mozilla focused on usability.&lt;p&gt;(Slide aside, it&amp;#x27;s not a coincidence the best CEO Mozilla ever had was a designer.)&lt;p&gt;I&amp;#x27;m not an AI hater, but I don&amp;#x27;t think Mozilla can compete here. There&amp;#x27;s just too much good stuff already, and it&amp;#x27;s not the type of thing Mozilla will shine with.&lt;p&gt;Instead, if I were CEO, I&amp;#x27;d go the opposite way: I&amp;#x27;d focus on privacy. Not AI privacy, but privacy in general. Buy a really great email provider, and start to own &amp;quot;identity on the internet&amp;quot;. As there&amp;#x27;s more bots and less privacy, identity is going to be incredibly important over the years.. and right now, Google defacto owns identity. Make it free, but also give people a way to pay.&lt;p&gt;Would this work? I don&amp;#x27;t know. But like I said, it&amp;#x27;s not a job I envy.&lt;/p&gt;
                    &lt;/blockquote&gt;
                    &lt;p&gt;&lt;strong&gt;Points:&lt;/strong&gt; 456 | &lt;strong&gt;Author:&lt;/strong&gt; recvonline&lt;/p&gt;&lt;p&gt;&lt;a href="https://news.ycombinator.com/item?id=46288491"&gt;View All Comments&lt;/a&gt;&lt;/p&gt;</description><hn_points>456</hn_points><hn_author>recvonline</hn_author><hn_order>10</hn_order></item><item><title>AI will make formal verification go mainstream</title><link>https://news.ycombinator.com/item?id=46294574</link><guid isPermaLink="true">https://news.ycombinator.com/item?id=46294574</guid><pubDate>Tue, 16 Dec 2025 21:14:49 +0000</pubDate><description>&lt;p&gt;&lt;strong&gt;Source:&lt;/strong&gt; &lt;a href="https://martin.kleppmann.com/2025/12/08/ai-formal-verification.html"&gt;martin.kleppmann.com&lt;/a&gt;&lt;/p&gt;
                    &lt;hr/&gt;
                    &lt;div&gt;&lt;b&gt;Top Comment:&lt;/b&gt;&lt;/div&gt;
                    &lt;blockquote style="margin:1.5em 0; padding:1em 1.5em; border-left:4px solid; font-style:italic;"&gt;
                    &lt;p&gt;I don&amp;#x27;t think formal verification really addresses most day-to-day programming problems:&lt;p&gt;&lt;pre&gt;&lt;code&gt;    * A user interface is confusing, or the English around it is unclear
    * An API you rely on changes, is deprecated, etc.
    * Users use something in unexpected ways
    * Updates forced by vendors or open source projects cause things to break
    * The customer isn&amp;#x27;t clear what they want
    * Complex behavior between interconnected systems, out of the purview of the formal language (OS + database + network + developer + VM + browser + user + web server)
&lt;/code&gt;&lt;/pre&gt;
For some mathematically pure task, sure, it&amp;#x27;s great. Or a low-level library like a regular expression parser or a compression codec. But I don&amp;#x27;t think that represents a lot of what most of us are tasked with, and those low-level &amp;quot;mathematically pure&amp;quot; libraries are generally pretty well handled by now.&lt;/p&gt;
                    &lt;/blockquote&gt;
                    &lt;p&gt;&lt;strong&gt;Points:&lt;/strong&gt; 434 | &lt;strong&gt;Author:&lt;/strong&gt; evankhoury&lt;/p&gt;&lt;p&gt;&lt;a href="https://news.ycombinator.com/item?id=46294574"&gt;View All Comments&lt;/a&gt;&lt;/p&gt;</description><hn_points>434</hn_points><hn_author>evankhoury</hn_author></item><item><title>40 percent of fMRI signals do not correspond to actual brain activity</title><link>https://news.ycombinator.com/item?id=46288415</link><guid isPermaLink="true">https://news.ycombinator.com/item?id=46288415</guid><pubDate>Tue, 16 Dec 2025 13:46:57 +0000</pubDate><description>&lt;p&gt;&lt;strong&gt;Source:&lt;/strong&gt; &lt;a href="https://www.tum.de/en/news-and-events/all-news/press-releases/details/40-percent-of-mri-signals-do-not-correspond-to-actual-brain-activity"&gt;www.tum.de&lt;/a&gt;&lt;/p&gt;
                    &lt;hr/&gt;
                    &lt;div&gt;&lt;b&gt;Top Comment:&lt;/b&gt;&lt;/div&gt;
                    &lt;blockquote style="margin:1.5em 0; padding:1em 1.5em; border-left:4px solid; font-style:italic;"&gt;
                    &lt;p&gt;My previous job was at a startup doing BMI, for research. For the first time I had the chance to work with expensive neural signal measurement tools (mainly EEG for us, but some teams used fMRI). and quickly did I learn how absolute horrible the signal to noise ratio (SNR) was in this field.&lt;p&gt;And how it was almost impossible to reproduce many published and well cited result. It was both exciting and jarring to talk with the neuroscientist, because they ofc knew about this and knew how to read the papers but the one doing more funding&amp;#x2F;business side ofc didn&amp;#x27;t really spend much time putting emphasis on that.&lt;p&gt;One of the team presented a accepted paper that basically used Deep Learning (Attention) to predict images that a person was thinking of, from the fMRI signals. When I asked &amp;quot;but DL is proven to be able to find pattern even in random noise, so how can you be sure this is not just overfitting to artefact?&amp;quot; and there wasn&amp;#x27;t really any answer to that (or rather the publication didn&amp;#x27;t take that in to account, although that can be experimentally determined). Still, a month later I saw tech explore or some tech news writing an article about it, something like &amp;quot;AI can now read your brain&amp;quot; and the 1984 implications yada yada.&lt;p&gt;So this is indeed something probably most practitioners, masters and PhD, realize relatively early.&lt;p&gt;So now that someone says &amp;quot;you know mindfulness is proven to change your brainwaves?&amp;quot; I always add my story &amp;quot;yes, but the study was done with EEG, so I don&amp;#x27;t trust the scientific backing of it&amp;quot; (but anecdotally, it helps me)&lt;/p&gt;
                    &lt;/blockquote&gt;
                    &lt;p&gt;&lt;strong&gt;Points:&lt;/strong&gt; 416 | &lt;strong&gt;Author:&lt;/strong&gt; geox&lt;/p&gt;&lt;p&gt;&lt;a href="https://news.ycombinator.com/item?id=46288415"&gt;View All Comments&lt;/a&gt;&lt;/p&gt;</description><hn_points>416</hn_points><hn_author>geox</hn_author><hn_order>9</hn_order></item><item><title>Announcing the Beta release of ty</title><link>https://news.ycombinator.com/item?id=46294289</link><guid isPermaLink="true">https://news.ycombinator.com/item?id=46294289</guid><pubDate>Tue, 16 Dec 2025 20:52:45 +0000</pubDate><description>&lt;p&gt;&lt;strong&gt;Source:&lt;/strong&gt; &lt;a href="https://astral.sh/blog/ty"&gt;astral.sh&lt;/a&gt;&lt;/p&gt;
                    &lt;hr/&gt;
                    &lt;div&gt;&lt;b&gt;Top Comment:&lt;/b&gt;&lt;/div&gt;
                    &lt;blockquote style="margin:1.5em 0; padding:1em 1.5em; border-left:4px solid; font-style:italic;"&gt;
                    &lt;p&gt;Hopefully it gets added to this comparison:&lt;p&gt;&lt;a href="https:&amp;#x2F;&amp;#x2F;htmlpreview.github.io&amp;#x2F;?https:&amp;#x2F;&amp;#x2F;github.com&amp;#x2F;python&amp;#x2F;typing&amp;#x2F;blob&amp;#x2F;main&amp;#x2F;conformance&amp;#x2F;results&amp;#x2F;results.html" rel="nofollow"&gt;https:&amp;#x2F;&amp;#x2F;htmlpreview.github.io&amp;#x2F;?https:&amp;#x2F;&amp;#x2F;github.com&amp;#x2F;python&amp;#x2F;typ...&lt;/a&gt;&lt;p&gt;If that table is anything to go by, Pyright is not to be underestimated.&lt;p&gt;I have briefly tried ty (LSP) in Emacs and it seems to work well so far. The only questionable thing I&amp;#x27;ve encountered is that when the signature of a method is shown, the type annotations of some parameters seem to be presented in a particularly verbose form compared to what I&amp;#x27;m used to - maybe they&amp;#x27;re technically correct but it can be bit much to look at.&lt;p&gt;Anyway, odds are pretty good that ty is what I will end up using long-term, so thanks and congrats on releasing the first beta!&lt;/p&gt;
                    &lt;/blockquote&gt;
                    &lt;p&gt;&lt;strong&gt;Points:&lt;/strong&gt; 411 | &lt;strong&gt;Author:&lt;/strong&gt; gavide&lt;/p&gt;&lt;p&gt;&lt;a href="https://news.ycombinator.com/item?id=46294289"&gt;View All Comments&lt;/a&gt;&lt;/p&gt;</description><hn_points>411</hn_points><hn_author>gavide</hn_author><hn_order>3</hn_order></item><item><title>Thin desires are eating life</title><link>https://news.ycombinator.com/item?id=46283276</link><guid isPermaLink="true">https://news.ycombinator.com/item?id=46283276</guid><pubDate>Tue, 16 Dec 2025 00:50:41 +0000</pubDate><description>&lt;p&gt;&lt;strong&gt;Source:&lt;/strong&gt; &lt;a href="https://www.joanwestenberg.com/thin-desires-are-eating-your-life/"&gt;www.joanwestenberg.com&lt;/a&gt;&lt;/p&gt;
                    &lt;hr/&gt;
                    &lt;div&gt;&lt;b&gt;Top Comment:&lt;/b&gt;&lt;/div&gt;
                    &lt;blockquote style="margin:1.5em 0; padding:1em 1.5em; border-left:4px solid; font-style:italic;"&gt;
                    &lt;p&gt;I can&amp;#x27;t help but feel that this article was written in a format that is the textual equivalent of thin desires…&lt;p&gt;Every sentence is separated into its own paragraph, like each one is supposed to be revelatory (or maybe tweet-worthy). It&amp;#x27;s pretty common design knowledge that if you try to emphasize everything, you end up emphasizing nothing. The result is that reading the article feels choppy, and weirdly unsatisfying, since the larger arc of each point is constantly being interrupted.&lt;p&gt;Why choose such an antithetical form, to what is otherwise an important and deep message?&lt;p&gt;The only answer that comes to mind is that the author&amp;#x27;s livelihood, or at least their internal gauge of success, is tied to manipulating readers&amp;#x27; thin desires.&lt;/p&gt;
                    &lt;/blockquote&gt;
                    &lt;p&gt;&lt;strong&gt;Points:&lt;/strong&gt; 381 | &lt;strong&gt;Author:&lt;/strong&gt; mitchbob&lt;/p&gt;&lt;p&gt;&lt;a href="https://news.ycombinator.com/item?id=46283276"&gt;View All Comments&lt;/a&gt;&lt;/p&gt;</description><hn_points>381</hn_points><hn_author>mitchbob</hn_author><hn_order>15</hn_order></item><item><title>GPT Image 1.5</title><link>https://news.ycombinator.com/item?id=46291941</link><guid isPermaLink="true">https://news.ycombinator.com/item?id=46291941</guid><pubDate>Tue, 16 Dec 2025 18:07:07 +0000</pubDate><description>&lt;p&gt;&lt;strong&gt;Source:&lt;/strong&gt; &lt;a href="https://openai.com/index/new-chatgpt-images-is-here/"&gt;openai.com&lt;/a&gt;&lt;/p&gt;&lt;p&gt;&lt;a href="https:&amp;#x2F;&amp;#x2F;platform.openai.com&amp;#x2F;docs&amp;#x2F;models&amp;#x2F;gpt-image-1.5" rel="nofollow"&gt;https:&amp;#x2F;&amp;#x2F;platform.openai.com&amp;#x2F;docs&amp;#x2F;models&amp;#x2F;gpt-image-1.5&lt;/a&gt;&lt;/p&gt;
                    &lt;hr/&gt;
                    &lt;div&gt;&lt;b&gt;Top Comment:&lt;/b&gt;&lt;/div&gt;
                    &lt;blockquote style="margin:1.5em 0; padding:1em 1.5em; border-left:4px solid; font-style:italic;"&gt;
                    &lt;p&gt;Okay results are in for GenAI Showdown with the new gpt-image 1.5 model for the editing portions of the site!&lt;p&gt;&lt;a href="https:&amp;#x2F;&amp;#x2F;genai-showdown.specr.net&amp;#x2F;image-editing" rel="nofollow"&gt;https:&amp;#x2F;&amp;#x2F;genai-showdown.specr.net&amp;#x2F;image-editing&lt;/a&gt;&lt;p&gt;Conclusions&lt;p&gt;- OpenAI has always had some of the strongest prompt understanding alongside the weakest image fidelity. This update goes some way towards addressing this weakness.&lt;p&gt;- It&amp;#x27;s leagues better at making localized edits without altering the entire image&amp;#x27;s aesthetic than gpt-image-1, doubling the previous score from 4&amp;#x2F;12 to 8&amp;#x2F;12 and the only model that &lt;i&gt;legitimately passed the Giraffe prompt&lt;/i&gt;.&lt;p&gt;- It&amp;#x27;s one of the most steerable models with a 90% compliance rate&lt;p&gt;Updates to GenAI Showdown&lt;p&gt;- Added outtakes sections to each model&amp;#x27;s detailed report in the Text-to-Image category, showcasing notable failures and unexpected behaviors.&lt;p&gt;- New models have been added including REVE and Flux.2 Dev (a new locally hostable model).&lt;p&gt;- Finally got around to implementing a weighted scoring mechanism which considers pass&amp;#x2F;fail, quality, and compliance for a more holistic model evaluation (click pass&amp;#x2F;fail icon to toggle between scoring methods).&lt;p&gt;If you just want to compare gpt-image-1, gpt-image-1.5, and NB Pro at the same time:&lt;p&gt;&lt;a href="https:&amp;#x2F;&amp;#x2F;genai-showdown.specr.net&amp;#x2F;image-editing?models=o4,nbp,g15" rel="nofollow"&gt;https:&amp;#x2F;&amp;#x2F;genai-showdown.specr.net&amp;#x2F;image-editing?models=o4,nbp...&lt;/a&gt;&lt;/p&gt;
                    &lt;/blockquote&gt;
                    &lt;p&gt;&lt;strong&gt;Points:&lt;/strong&gt; 365 | &lt;strong&gt;Author:&lt;/strong&gt; charlierguo&lt;/p&gt;&lt;p&gt;&lt;a href="https://news.ycombinator.com/item?id=46291941"&gt;View All Comments&lt;/a&gt;&lt;/p&gt;</description><hn_points>365</hn_points><hn_author>charlierguo</hn_author><hn_order>5</hn_order></item><item><title>Sega Channel: VGHF Recovers over 100 Sega Channel ROMs (and More)</title><link>https://news.ycombinator.com/item?id=46288024</link><guid isPermaLink="true">https://news.ycombinator.com/item?id=46288024</guid><pubDate>Tue, 16 Dec 2025 13:07:14 +0000</pubDate><description>&lt;p&gt;&lt;strong&gt;Source:&lt;/strong&gt; &lt;a href="https://gamehistory.org/segachannel/"&gt;gamehistory.org&lt;/a&gt;&lt;/p&gt;
                    &lt;hr/&gt;
                    &lt;div&gt;&lt;b&gt;Top Comment:&lt;/b&gt;&lt;/div&gt;
                    &lt;blockquote style="margin:1.5em 0; padding:1em 1.5em; border-left:4px solid; font-style:italic;"&gt;
                    &lt;p&gt;One of the most interesting things in this release IMO is the internal documents from Sega Channel management. For example, in this binder &lt;a href="https:&amp;#x2F;&amp;#x2F;archive.gamehistory.org&amp;#x2F;item&amp;#x2F;ef6246e4-79be-4b02-b262-af2881fdc16d" rel="nofollow"&gt;https:&amp;#x2F;&amp;#x2F;archive.gamehistory.org&amp;#x2F;item&amp;#x2F;ef6246e4-79be-4b02-b262...&lt;/a&gt; there&amp;#x27;s a bunch of research documents trying to figure out how to turn the service around after it began underperforming their expectations.&lt;p&gt;It seems like the main problem they ran into was that the service appealed mainly to the small minority of &amp;quot;heavy players&amp;quot; (who they defined as playing more than 14 hours per week). Their original projections were that they could target cable subscribers who own Genesis systems and play games more than 4 hours a week, but they found that most people who weren&amp;#x27;t gaming fanatics preferred to own a few games and rent games as needed rather than subscribe to Sega Channel.&lt;p&gt;The other big problem they ran into was parental resistance. A large amount of parents they talked to viewed Sega Channel as an &amp;quot;open tap&amp;quot; that would increase their child&amp;#x27;s time spent playing games. An ongoing subscription also was only a one-time &amp;quot;give&amp;quot; from the parent to the child, whereas buying&amp;#x2F;renting games was one &amp;quot;give&amp;quot; per occasion, which was more psychologically attractive to the parents.&lt;/p&gt;
                    &lt;/blockquote&gt;
                    &lt;p&gt;&lt;strong&gt;Points:&lt;/strong&gt; 233 | &lt;strong&gt;Author:&lt;/strong&gt; wicket&lt;/p&gt;&lt;p&gt;&lt;a href="https://news.ycombinator.com/item?id=46288024"&gt;View All Comments&lt;/a&gt;&lt;/p&gt;</description><hn_points>233</hn_points><hn_author>wicket</hn_author><hn_order>21</hn_order></item><item><title>Purrtran – ᓚᘏᗢ – A Programming Language for Cat People</title><link>https://news.ycombinator.com/item?id=46256504</link><guid isPermaLink="true">https://news.ycombinator.com/item?id=46256504</guid><pubDate>Sat, 13 Dec 2025 18:02:46 +0000</pubDate><description>&lt;p&gt;&lt;strong&gt;Source:&lt;/strong&gt; &lt;a href="https://github.com/cmontella/purrtran"&gt;github.com&lt;/a&gt;&lt;/p&gt;
                    &lt;hr/&gt;
                    &lt;div&gt;&lt;b&gt;Top Comment:&lt;/b&gt;&lt;/div&gt;
                    &lt;blockquote style="margin:1.5em 0; padding:1em 1.5em; border-left:4px solid; font-style:italic;"&gt;
                    &lt;p&gt;&amp;gt; PURRTRAN allocates all variables to an arena called the &amp;quot;Litterbox&amp;quot;. The Litterbox must be manually emptied at least once a day by the user, or Hex&amp;#x27;s cleanliness and love will decrease. The Litterbox can overflow, which will cause Hex to become very displeased and may lead to unexpected program behavior, as Hex will begin storing variables in your source code text buffer instead of the Litterbox until it&amp;#x27;s cleaned.&lt;p&gt;I&amp;#x27;m cackling like a madman, thank you for this op.&lt;/p&gt;
                    &lt;/blockquote&gt;
                    &lt;p&gt;&lt;strong&gt;Points:&lt;/strong&gt; 229 | &lt;strong&gt;Author:&lt;/strong&gt; simonpure&lt;/p&gt;&lt;p&gt;&lt;a href="https://news.ycombinator.com/item?id=46256504"&gt;View All Comments&lt;/a&gt;&lt;/p&gt;</description><hn_points>229</hn_points><hn_author>simonpure</hn_author><hn_order>41</hn_order></item><item><title>Bonsai: A Voxel Engine, from scratch</title><link>https://news.ycombinator.com/item?id=46285319</link><guid isPermaLink="true">https://news.ycombinator.com/item?id=46285319</guid><pubDate>Tue, 16 Dec 2025 06:06:43 +0000</pubDate><description>&lt;p&gt;&lt;strong&gt;Source:&lt;/strong&gt; &lt;a href="https://github.com/scallyw4g/bonsai"&gt;github.com&lt;/a&gt;&lt;/p&gt;
                    &lt;hr/&gt;
                    &lt;div&gt;&lt;b&gt;Top Comment:&lt;/b&gt;&lt;/div&gt;
                    &lt;blockquote style="margin:1.5em 0; padding:1em 1.5em; border-left:4px solid; font-style:italic;"&gt;
                    &lt;p&gt;The author mentions simplicity in their Readme. I would be very interested to read their journey and some of the decisions they made where they preferred simplicity. More of this please !&lt;/p&gt;
                    &lt;/blockquote&gt;
                    &lt;p&gt;&lt;strong&gt;Points:&lt;/strong&gt; 221 | &lt;strong&gt;Author:&lt;/strong&gt; jesse__&lt;/p&gt;&lt;p&gt;&lt;a href="https://news.ycombinator.com/item?id=46285319"&gt;View All Comments&lt;/a&gt;&lt;/p&gt;</description><hn_points>221</hn_points><hn_author>jesse__</hn_author><hn_order>48</hn_order></item><item><title>Full Unicode Search at 50× ICU Speed with AVX‑512</title><link>https://news.ycombinator.com/item?id=46276826</link><guid isPermaLink="true">https://news.ycombinator.com/item?id=46276826</guid><pubDate>Mon, 15 Dec 2025 16:42:55 +0000</pubDate><description>&lt;p&gt;&lt;strong&gt;Source:&lt;/strong&gt; &lt;a href="https://ashvardanian.com/posts/search-utf8/"&gt;ashvardanian.com&lt;/a&gt;&lt;/p&gt;
                    &lt;hr/&gt;
                    &lt;div&gt;&lt;b&gt;Top Comment:&lt;/b&gt;&lt;/div&gt;
                    &lt;blockquote style="margin:1.5em 0; padding:1em 1.5em; border-left:4px solid; font-style:italic;"&gt;
                    &lt;p&gt;I was really confused about the case folding, this page explained the motivation well &lt;a href="https:&amp;#x2F;&amp;#x2F;jean.abou-samra.fr&amp;#x2F;blog&amp;#x2F;unicode-misconceptions" rel="nofollow"&gt;https:&amp;#x2F;&amp;#x2F;jean.abou-samra.fr&amp;#x2F;blog&amp;#x2F;unicode-misconceptions&lt;/a&gt;&lt;p&gt;&amp;quot;&amp;quot;&amp;quot;
Continuing with the previous example of “ß”, one has lowercase(&amp;quot;ss&amp;quot;) != lowercase(&amp;quot;ß&amp;quot;) but uppercase(&amp;quot;ss&amp;quot;) == uppercase(&amp;quot;ß&amp;quot;). Conversely, for legacy reasons (compatibility with encodings predating Unicode), there exists a Kelvin sign “K”, which is distinct from the Latin uppercase letter “K”, but also lowercases to the normal Latin lowercase letter “k”, so that uppercase(&amp;quot;K&amp;quot;) != uppercase(&amp;quot;K&amp;quot;) but lowercase(&amp;quot;K&amp;quot;) == lowercase(&amp;quot;K&amp;quot;).&lt;p&gt;The correct way is to use Unicode case folding, a form of normalization designed specifically for case-insensitive comparisons. Both casefold(&amp;quot;ß&amp;quot;) == casefold(&amp;quot;ss&amp;quot;) and casefold(&amp;quot;K&amp;quot;) == casefold(&amp;quot;K&amp;quot;) are true. Case folding usually yields the same result as lowercasing, but not always (e.g., “ß” lowercases to itself but case-folds to “ss”).
&amp;quot;&amp;quot;&amp;quot;&lt;p&gt;One question I have is why have Kelvin sign that is distinct from Latin K and other indistinguishable symbols? To make quantified machine readable (oh, this is not a 100K license plate or money amount, but a temperature)? Or to make it easier for specialized software to display it in correct placed&amp;#x2F;units?&lt;/p&gt;
                    &lt;/blockquote&gt;
                    &lt;p&gt;&lt;strong&gt;Points:&lt;/strong&gt; 196 | &lt;strong&gt;Author:&lt;/strong&gt; ashvardanian&lt;/p&gt;&lt;p&gt;&lt;a href="https://news.ycombinator.com/item?id=46276826"&gt;View All Comments&lt;/a&gt;&lt;/p&gt;</description><hn_points>196</hn_points><hn_author>ashvardanian</hn_author><hn_order>34</hn_order></item><item><title>No AI* Here – A Response to Mozilla's Next Chapter</title><link>https://news.ycombinator.com/item?id=46295268</link><guid isPermaLink="true">https://news.ycombinator.com/item?id=46295268</guid><pubDate>Tue, 16 Dec 2025 22:07:49 +0000</pubDate><description>&lt;p&gt;&lt;strong&gt;Source:&lt;/strong&gt; &lt;a href="https://www.waterfox.com/blog/no-ai-here-response-to-mozilla/"&gt;www.waterfox.com&lt;/a&gt;&lt;/p&gt;
                    &lt;hr/&gt;
                    &lt;div&gt;&lt;b&gt;Top Comment:&lt;/b&gt;&lt;/div&gt;
                    &lt;blockquote style="margin:1.5em 0; padding:1em 1.5em; border-left:4px solid; font-style:italic;"&gt;
                    &lt;p&gt;&amp;gt; Large language models are something else entirely*. They are black boxes. You cannot audit them. You cannot truly understand what they do with your data. You cannot verify their behaviour. And Mozilla wants to put them at the heart of the browser and that doesn&amp;#x27;t sit well.&lt;p&gt;Am I being overly critical here or is this kind of a silly position to have right after talking about how neural machine translation is okay? Many of Firefox&amp;#x27;s LLM features like summarization afaik are powered by local models (hell even Chrome has local model options). It&amp;#x27;s weird to say neural translation is not a black box but LLMs are somehow black boxes that we cannot hope to understand what they do with the data, especially when viewed a bit fuzzily LLMs are scaled up versions of an architecture that was originally used for neural translation. Neural translation also has unverifiable behavior in the same sense.&lt;p&gt;I could interpret some of the data talk as talking about non local models but this very much seems like a more general criticism of LLMs as a whole when talking about Firefox features. Moreover, some of the critiques like verifiability of outputs and unlimited scope still don&amp;#x27;t make sense in this context. Browser LLM features except for explicitly AI browsers like Comet have so far had some scoping to their behavior, either in very narrow scopes like translation or summarization. The broadest scope I can think of is the side panels that show up which allow you to ask about a web page with context. Even then, I do not see what is inherently problematic about such scoping since the output behavior is confined to the side panel.&lt;/p&gt;
                    &lt;/blockquote&gt;
                    &lt;p&gt;&lt;strong&gt;Points:&lt;/strong&gt; 177 | &lt;strong&gt;Author:&lt;/strong&gt; MrAlex94&lt;/p&gt;&lt;p&gt;&lt;a href="https://news.ycombinator.com/item?id=46295268"&gt;View All Comments&lt;/a&gt;&lt;/p&gt;</description><hn_points>177</hn_points><hn_author>MrAlex94</hn_author><hn_order>11</hn_order></item><item><title>Rust GCC backend: Why and how</title><link>https://news.ycombinator.com/item?id=46288291</link><guid isPermaLink="true">https://news.ycombinator.com/item?id=46288291</guid><pubDate>Tue, 16 Dec 2025 13:33:25 +0000</pubDate><description>&lt;p&gt;&lt;strong&gt;Source:&lt;/strong&gt; &lt;a href="https://blog.guillaume-gomez.fr/articles/2025-12-15+Rust+GCC+backend%3A+Why+and+how"&gt;blog.guillaume-gomez.fr&lt;/a&gt;&lt;/p&gt;
                    &lt;hr/&gt;
                    &lt;div&gt;&lt;b&gt;Top Comment:&lt;/b&gt;&lt;/div&gt;
                    &lt;blockquote style="margin:1.5em 0; padding:1em 1.5em; border-left:4px solid; font-style:italic;"&gt;
                    &lt;p&gt;&amp;gt; On that note: GCC doesn&amp;#x27;t provide a nice library to give access to its internals (unlike LLVM). So we have to use libgccjit which, unlike the &amp;quot;jit&amp;quot; (&amp;quot;just in time&amp;quot;, meaning compiling sub-parts of the code on the fly, only when needed for performance reasons and often used in script languages like Javascript) part in its name implies, can be used as &amp;quot;aot&amp;quot; (&amp;quot;ahead of time&amp;quot;, meaning you compile everything at once, allowing you to spend more time on optimization).&lt;p&gt;Is libgccjit not “a nice library to give access to its internals?”&lt;/p&gt;
                    &lt;/blockquote&gt;
                    &lt;p&gt;&lt;strong&gt;Points:&lt;/strong&gt; 173 | &lt;strong&gt;Author:&lt;/strong&gt; ahlCVA&lt;/p&gt;&lt;p&gt;&lt;a href="https://news.ycombinator.com/item?id=46288291"&gt;View All Comments&lt;/a&gt;&lt;/p&gt;</description><hn_points>173</hn_points><hn_author>ahlCVA</hn_author><hn_order>31</hn_order></item><item><title>Nvidia Nemotron 3 Family of Models</title><link>https://news.ycombinator.com/item?id=46275111</link><guid isPermaLink="true">https://news.ycombinator.com/item?id=46275111</guid><pubDate>Mon, 15 Dec 2025 14:39:49 +0000</pubDate><description>&lt;p&gt;&lt;strong&gt;Source:&lt;/strong&gt; &lt;a href="https://research.nvidia.com/labs/nemotron/Nemotron-3/"&gt;research.nvidia.com&lt;/a&gt;&lt;/p&gt;
                    &lt;hr/&gt;
                    &lt;div&gt;&lt;b&gt;Top Comment:&lt;/b&gt;&lt;/div&gt;
                    &lt;blockquote style="margin:1.5em 0; padding:1em 1.5em; border-left:4px solid; font-style:italic;"&gt;
                    &lt;p&gt;I don’t do ‘evals’, but I do process billions of tokens every month, and I’ve found these small Nvidia models to be the best by far for their size currently.&lt;p&gt;As someone else mentioned, the GPT-OSS models are also quite good (though I haven’t found how to make them &lt;i&gt;great&lt;/i&gt; yet, though I think they might age well like the Llama 3 models did and get better with time!).&lt;p&gt;But for a defined task, I’ve found task compliance, understanding, and tool call success rates to be some of the highest on these Nvidia models.&lt;p&gt;For example, I have a continuous job that evaluates if the data for a startup company on aVenture.vc could have overlapping&amp;#x2F;conflated two similar but unrelated companies for news articles, research details, investment rounds, etc… which is a token hungry ETL task! And I recently retested this workflow on the top 15 or so models today with &amp;lt;125b parameters, and the Nvidia models were among the best performing for this type of work, particularly around non-hallucination if given adequate grounding.&lt;p&gt;Also, re: cost - I run local inference on several machines that run continuously, in addition to routing through OpenRouter and the frontier providers, and was pleasantly surprised to find that if I’m a paying customer of OpenRouter otherwise, the free variant there from Nvidia is quite generous for limits, too.&lt;/p&gt;
                    &lt;/blockquote&gt;
                    &lt;p&gt;&lt;strong&gt;Points:&lt;/strong&gt; 164 | &lt;strong&gt;Author:&lt;/strong&gt; ewt-nv&lt;/p&gt;&lt;p&gt;&lt;a href="https://news.ycombinator.com/item?id=46275111"&gt;View All Comments&lt;/a&gt;&lt;/p&gt;</description><hn_points>164</hn_points><hn_author>ewt-nv</hn_author><hn_order>23</hn_order></item><item><title>30 years of &lt;br&gt; tags</title><link>https://news.ycombinator.com/item?id=46254794</link><guid isPermaLink="true">https://news.ycombinator.com/item?id=46254794</guid><pubDate>Sat, 13 Dec 2025 14:35:28 +0000</pubDate><description>&lt;p&gt;&lt;strong&gt;Source:&lt;/strong&gt; &lt;a href="https://www.artmann.co/articles/30-years-of-br-tags"&gt;www.artmann.co&lt;/a&gt;&lt;/p&gt;
                    &lt;hr/&gt;
                    &lt;div&gt;&lt;b&gt;Top Comment:&lt;/b&gt;&lt;/div&gt;
                    &lt;blockquote style="margin:1.5em 0; padding:1em 1.5em; border-left:4px solid; font-style:italic;"&gt;
                    &lt;p&gt;&amp;gt; Every page on your site needed the same header, the same navigation, the same footer. But there was no way to share these elements.  No includes, no components.&lt;p&gt;That&amp;#x27;s not completely true.  Webservers have Server Side Includes (SSI) [0].  Also if you don&amp;#x27;t want to rely on that, &amp;#x27;cat header body &amp;gt; file&amp;#x27; isn&amp;#x27;t really that hard.&lt;p&gt;[0] &lt;a href="https:&amp;#x2F;&amp;#x2F;web.archive.org&amp;#x2F;web&amp;#x2F;19970303194503&amp;#x2F;http:&amp;#x2F;&amp;#x2F;hoohoo.ncsa.uiuc.edu&amp;#x2F;docs&amp;#x2F;tutorials&amp;#x2F;includes.html" rel="nofollow"&gt;https:&amp;#x2F;&amp;#x2F;web.archive.org&amp;#x2F;web&amp;#x2F;19970303194503&amp;#x2F;http:&amp;#x2F;&amp;#x2F;hoohoo.ncs...&lt;/a&gt;&lt;/p&gt;
                    &lt;/blockquote&gt;
                    &lt;p&gt;&lt;strong&gt;Points:&lt;/strong&gt; 162 | &lt;strong&gt;Author:&lt;/strong&gt; FragrantRiver&lt;/p&gt;&lt;p&gt;&lt;a href="https://news.ycombinator.com/item?id=46254794"&gt;View All Comments&lt;/a&gt;&lt;/p&gt;</description><hn_points>162</hn_points><hn_author>FragrantRiver</hn_author><hn_order>33</hn_order></item><item><title>A2UI: A Protocol for Agent-Driven Interfaces</title><link>https://news.ycombinator.com/item?id=46286407</link><guid isPermaLink="true">https://news.ycombinator.com/item?id=46286407</guid><pubDate>Tue, 16 Dec 2025 09:16:31 +0000</pubDate><description>&lt;p&gt;&lt;strong&gt;Source:&lt;/strong&gt; &lt;a href="https://a2ui.org/"&gt;a2ui.org&lt;/a&gt;&lt;/p&gt;
                    &lt;hr/&gt;
                    &lt;div&gt;&lt;b&gt;Top Comment:&lt;/b&gt;&lt;/div&gt;
                    &lt;blockquote style="margin:1.5em 0; padding:1em 1.5em; border-left:4px solid; font-style:italic;"&gt;
                    &lt;p&gt;&amp;gt; A2UI lets agents send declarative component descriptions that clients render using their own native widgets. It&amp;#x27;s like having agents speak a &lt;i&gt;universal UI language&lt;/i&gt;.&lt;p&gt;(emphasis mine)&lt;p&gt;Sounds like agents are suddenly able to do what developers have failed at for decades: Writing platform-independent UIs. Maybe this works for simple use cases but beyond that I&amp;#x27;m skeptical.&lt;/p&gt;
                    &lt;/blockquote&gt;
                    &lt;p&gt;&lt;strong&gt;Points:&lt;/strong&gt; 147 | &lt;strong&gt;Author:&lt;/strong&gt; makeramen&lt;/p&gt;&lt;p&gt;&lt;a href="https://news.ycombinator.com/item?id=46286407"&gt;View All Comments&lt;/a&gt;&lt;/p&gt;</description><hn_points>147</hn_points><hn_author>makeramen</hn_author><hn_order>51</hn_order></item><item><title>Japan to revise romanization rules for first time in 70 years</title><link>https://news.ycombinator.com/item?id=46286292</link><guid isPermaLink="true">https://news.ycombinator.com/item?id=46286292</guid><pubDate>Tue, 16 Dec 2025 08:54:58 +0000</pubDate><description>&lt;p&gt;&lt;strong&gt;Source:&lt;/strong&gt; &lt;a href="https://www.japantimes.co.jp/news/2025/08/21/japan/panel-hepburn-style-romanization/"&gt;www.japantimes.co.jp&lt;/a&gt;&lt;/p&gt;
                    &lt;hr/&gt;
                    &lt;div&gt;&lt;b&gt;Top Comment:&lt;/b&gt;&lt;/div&gt;
                    &lt;blockquote style="margin:1.5em 0; padding:1em 1.5em; border-left:4px solid; font-style:italic;"&gt;
                    &lt;p&gt;Curiously enough, Hepburn romanization fixes some ambiguities in Japanese (Japanese written in kana alone) while introducing others.&lt;p&gt;The ō in Hepburn could correspond to おう or おお or オー. That&amp;#x27;s an ambiguity.&lt;p&gt;Where does Hepburn disambiguate?&lt;p&gt;In Japanese, an E column kana followed by I sometimes makes a long E, like in 先生 (sen + sei -&amp;gt; sensē). The &amp;quot;SEI&amp;quot; is one unit.  But in other situations it does not, like in a compound word ending in the E kana, where the second word starts with I. For instance 酒色 (sake + iro -&amp;gt; sakeiro, not sakēro).&lt;p&gt;Hepburn distinguishes these; the hiragana spelling does not!&lt;p&gt;This is one of the issues that makes it very hard to read Japanese that is written with hiragana only, rather than kanji.   No word breaks and not knowing whether せい is supposed to be sē or sei.&lt;p&gt;There are curiosities like karaage which is &amp;quot;kara&amp;quot; (crust) + &amp;quot;age&amp;quot; (fried thing). A lot of the time it is pronounced as karāge, because of the way RA and A come together. Other times you hear a kind of flutter in it which articulates two A&amp;#x27;s.&lt;p&gt;I have no idea which romanization to use. Flip a coin?&lt;/p&gt;
                    &lt;/blockquote&gt;
                    &lt;p&gt;&lt;strong&gt;Points:&lt;/strong&gt; 146 | &lt;strong&gt;Author:&lt;/strong&gt; rgovostes&lt;/p&gt;&lt;p&gt;&lt;a href="https://news.ycombinator.com/item?id=46286292"&gt;View All Comments&lt;/a&gt;&lt;/p&gt;</description><hn_points>146</hn_points><hn_author>rgovostes</hn_author><hn_order>19</hn_order></item><item><title>Vibe coding creates fatigue?</title><link>https://news.ycombinator.com/item?id=46292365</link><guid isPermaLink="true">https://news.ycombinator.com/item?id=46292365</guid><pubDate>Tue, 16 Dec 2025 18:32:46 +0000</pubDate><description>&lt;p&gt;&lt;strong&gt;Source:&lt;/strong&gt; &lt;a href="https://www.tabulamag.com/p/too-fast-to-think-the-hidden-fatigue"&gt;www.tabulamag.com&lt;/a&gt;&lt;/p&gt;
                    &lt;hr/&gt;
                    &lt;div&gt;&lt;b&gt;Top Comment:&lt;/b&gt;&lt;/div&gt;
                    &lt;blockquote style="margin:1.5em 0; padding:1em 1.5em; border-left:4px solid; font-style:italic;"&gt;
                    &lt;p&gt;I don&amp;#x27;t want to be that contrarian guy, but I find it energizing to go faster. For example, being able to blast through a list of niggling defects that need to be fixed is no longer a stultifying drag.&lt;p&gt;I recently used a coding agent on a project where I was using an unfamiliar language, framework, API, and protocol. It was a non-trivial project, and I had to be paying attention to what the agent was doing because it definitely would go off into the weeds fairly often. But not having to spend hours here and there getting up to speed on some mundane but unfamiliar aspect of the implementation really made everything about the experience better.&lt;p&gt;I even explored some aspects of LLM performance: I could tell that new and fast changing APIs easily flummox a coding agent, confirming the strong relationship of up-to-date and accurate training material to LLM performance. I&amp;#x27;ve also seen this aspect of agent assisted coding improve and vary across AIs.&lt;/p&gt;
                    &lt;/blockquote&gt;
                    &lt;p&gt;&lt;strong&gt;Points:&lt;/strong&gt; 146 | &lt;strong&gt;Author:&lt;/strong&gt; rom16384&lt;/p&gt;&lt;p&gt;&lt;a href="https://news.ycombinator.com/item?id=46292365"&gt;View All Comments&lt;/a&gt;&lt;/p&gt;</description><hn_points>146</hn_points><hn_author>rom16384</hn_author><hn_order>49</hn_order></item></channel></rss>